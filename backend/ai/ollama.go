package ai

import (
	"bytes"
	"encoding/json"
	"errors"
	"fmt"
	"net/http"
	"time"
)

type OllamaRequest struct {
	Model  string `json:"model"`
	Prompt string `json:"prompt"`
}

type OllamaResponse struct {
	Response string `json:"response"`
	Done     bool   `json:"done"`
}

// ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å Ollama local API ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°
func GenerateSummaryLocal(content string, timeframe string) (string, error) {
	prompt := fmt.Sprintf(`‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ö‡∏≤‡∏á‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô‡πÉ‡∏´‡πâ‡∏™‡∏£‡∏∏‡∏õ‡πÅ‡∏ö‡πà‡∏á‡∏≠‡∏≠‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡πÜ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î:`, timeframe, content)

	reqBody := OllamaRequest{
		Model:  "mistral",
		Prompt: prompt,
	}

	data, err := json.Marshal(reqBody)
	if err != nil {
		return "", err
	}

	client := http.Client{Timeout: 60 * time.Second}
	resp, err := client.Post("http://localhost:11434/api/generate", "application/json", bytes.NewBuffer(data))
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return "", errors.New("failed to call Ollama API: status " + resp.Status)
	}

	// üü¢ ‡∏≠‡πà‡∏≤‡∏ô response ‡∏ó‡∏µ‡∏•‡∏∞‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡πÅ‡∏ö‡∏ö streaming
	var result string
	decoder := json.NewDecoder(resp.Body)
	for decoder.More() {
		var res OllamaResponse
		if err := decoder.Decode(&res); err != nil {
			break
		}
		result += res.Response
		if res.Done {
			break
		}
	}

	return result, nil
}
